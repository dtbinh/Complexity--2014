\section{ Conclusions \& Future Work }\label{sec:conc}

Forecast strategies that are designed to capture predictive structure
are ineffective when signal complexity outweighs information
redundancy.  This poses a number of serious challenges in practice.
Without knowing anything about the generating process, it is difficult
to determine how much predictive structure is present in a given time
series.  And even if predictive structure exists, a given forecast
method may not work, simply because it cannot exploit the structure
that is present (e.g., a linear model of a nonlinear process).  If a
forecast model is not producing good results, a practitioner needs to
know why: is the reason that the time series contains no predictive
structure---i.e., that no model will work---or is the model that s/he
is using simply not good enough?

In this paper, we have argued that redundancy~\cite{crutchfield2003}
is a useful definition of the inherent predictability of a time
series.  To operationalize that definition, we use an approximation of
the Kolmogorov-Sinai entropy~\cite{lind95}, estimated using a weighted
version of the permutation entropy of~\cite{bandt2002per}.  This WPE
technique---an ordinal calculation of forward information transfer in
a time series---is ideal for our purposes because it works with
real-valued data and is known to converge to the true entropy value.
Using a variety of forecast models and more than a hundred time-series
data sets from a computer-performance experiment, we have shown that
prediction accuracy is indeed correlated with weighted permutation
entropy: the higher the WPE, in general, the higher the prediction
error.  There are some exceptions: \svdone is easier to predict than
its WPE value suggests using any of the three forecast strategies
studied here.  Conversely, forecast errors for the \col and \svdfive
signals are higher than the corresponding WPEs suggest---except for
the nonlinear LMA predictor.

% Further, the persistent WPE values of 0.5--0.6 for the {\tt
%   col\_major} trace are consistent with dynamical chaos, further
% corroborating the results of~\cite{mytkowicz09}.

An important practical corollary to this empirical correlation of
predictability and WPE is a practical strategy for assessing
appropriateness of forecast methods.  If the forecast produced by a
particular method is poor but the time series contains a significant
amount of predictive structure, one can reasonably conclude that that
method is inadequate to the task and that one should seek another
method.  The nonlinear LMA method, for instance, performs better in
most cases because it is more general.  (This is particularly apparent
in the \col and \svdfive examples.)
% , where the other two methods do not perform well.)  
The \naive ~method, which simply predicts the mean, can work very well
on noisy signals because it effects a filtering operation.  The simple
random-walk strategy outperforms LMA, ARIMA, and the \naive ~method on
the \gcc signal, which is extremely complex---i.e., extremely low
redundancy.
%  \naive ~wins on \svdone with the exception to the 5 outliers.
The dashed line in Figures~\ref{fig:wpe_vs_mase_best}
and~\ref{fig:wpe_vs_mase_all} operationalizes the discussion in this
paragraph.  It is a useful empirical guideline for knowing when a
model is not well-matched to the task at hand: a MASE score that is
more than $WPE = 0.2794 \cdot MASE + 0.5334$
times the WPE of the corresponding signal is an indication that that
time series has more predictive structure than that forecast model can
capture and exploit.

Given that information is a fundamental limitation in predictability,
then gathering and using more information is an obvious next step.
But there is an equally obvious tension here between data length and
prediction speed: a forecast that requires half a second to compute is
not useful for the purposes of real-time control of a computer system
with a MHz clock rate.  Another alternative is to sample several
system variables simultaneously and build multivariate models.  This
is a particular challenge in nonlinear LMA-type models, since
multivariate delay-coordinate embedding (e.g.,
\cite{cao-multivariate-embedding,deyle-sugihara2011}) can be
computationally prohibitive.  We are working on alternative methods
that sidestep that complexity.

%Paragraphs on the issues that come up, including one about the "amount
%of info" one: if one could sample more variables, for instance, one
%might be able to do a better job of predicting more-complex traces.
%Segue to some handwaving about multivariable LMA models; tie this back
%to the "computers are NLD systems" stuff in the intro.  This is a real
%challenge; current approaches to this modelling problem have the major
%issue of taking way too long to build.  And that's a big issue if
%you're trying not just to classify, but to predict.  In a system that
%runs at MHz speeds, a prediction that takes milliseconds to compute is
%not useful.
%\end{it}

Nonstationarity is a serious challenge in any time-series modeling
problem.  Indeed, one of the first applications of permutation entropy
was to recognize the regime shift in brainwave data that occurs when
someone has a seizure~\cite{cao2004det}.  Recall that the signal in
Figure~\ref{fig:svd-ts-colored} was especially useful for the study in
this paper because it contained a number of different regimes.  We
segmented this signal visually, but one could imagine using
permutation entropy to do so instead.  Automating regime-shift
detection would be an important step towards a fully adaptive modeling
strategy, where old models are discarded and new ones are rebuilt
whenever the time series enters a new regime.  WPE would be
particularly powerful in this scenario, as its value can not only help
with regime-shift detection, but also suggest what kind of model might
work well in each new regime.

All of the experiments described in this paper involve a particular
physical system.  We chose that system because its behavior spans a
broad spectrum of classical time series patterns.  Since these
patterns---dynamical chaos, periodicities, noise, etc.--- are generic,
we believe that our results will generalize beyond this particular
system.  Testing that claim with different examples is another
important next step.  Of particular interest would be the class of
so-called \emph{hybrid systems}: [[insert def]].  A lathe, for
instance, that has an intermittent instability.  Traffic at an
internet router, which has characteristic patterns during normal
operation that shift radically during an attack.  Modeling and
predicting the behavior of these kinds of systems effectively is quite
difficult; doing so adaptively and automatically---in the manner that
is alluded to at the end of the previous paragraph---would be an even
more important challenge.

% The way structure, information and complexity are processed by a
% generating process play a crucial role in the success of a given
% prediction scheme.
