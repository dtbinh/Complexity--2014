 \section{Predictability, Complexity, and Permutation Entropy} % (fold)
 \label{sec:results}
% I changed the title of this section because there are lots of
% results in the modeling section too.  The results HERE are about the
% relationship between predictability and WPE


%OUTLINE:
%\begin{enumerate}
%\item \cmark~Introduce MASE.
%\item \cmark~WPE is a good measure of predictability.  %Figure: best athlete MASE vs.
%its WPE.
%\item \cmark~Talk about structure analysis. Just because %there is forward information
%transfer, does not mean that linear predictors can get at %this.
%\subitem \cmark~For this show a figure of (a) ARIMA vs MASE %(b) LMA vs MASE in a side by%
%side plot.
%\item full results.  Image: MASE vs. WPE for both LMA \& %ARIMA.  Points to make:
%\begin{enumerate}
%\item \cmark~clusters are distributed differently
%\item \cmark~clusters are shaped differently---tight or not
%\item \cmark~clusters move differently between LMA and ARIMA
%\item finally, the diagonal line is important. If you're %below it, you could do
%better.
%\end{enumerate}
%\end{enumerate}

%BRAINSTORMING 
%\begin{itemize}
%\cmark\item The kind of complexity present matters, i.e., that is whether the complexity is structured or not.[[use here and mention in intro]] 
%\cmark\item Quantifying structured and unstructured complexity is nontrivial in the case of real-valued noisy time series but WPE does this. [[talk about again here but justify in information theory section]]
%\item Maybe plot a big chunk of \col and a big chunk of \gcc together and show that they both look complex. 

%\cmark\item \gcc appears visually very complex, *and* according to WPE this complexity is unstructured. And a constant, linear and nonlinear prediction strategy all fail. We should be able to conclude that guessing random values is the best we can do as is shown by MASE [[Use in this section]]

%\cmark\item \col is also complex (can even be chaotic/ point to CHAOS paper) but the complexity is structured according to WPE and as such that complexity is usable for prediction [[use in this section]]

%\cmark\item \col brings about the point nicely that some prediction strategies cannot utilize the processes internal information transfer method. That is a nonlinear internal information transfer system cannot be predicted effectively with a linear strategy. This gives a practitioner leverage on when to give up and when to keep working. [[use in this section as bridge to next section]]

%\end{itemize}

In this section, we offer an empirical validation of the two key
conjectures introduced in Section \ref{sec:intro}, namely:

\begin{enumerate}

\item that the weighted permutation entropy (WPE) of a noisy
  real-valued time series is correlated with prediction
  accuracy---i.e., that the predictable structure in a time-series
  data set can be quantified by its WPE.

%\subitem [[Simply reminders not to be included]]WPE can quantify when
%a noisy real-valued time series is predictable.[[I am unsatisfied
%with predictable in this sentence, need a better word to say "better
%than random walk" or ``able to be forecast effectively" or ``has the
%structural capacity to transmit information in a way that the time
%series can be effectively forecast"

%\item The existence of complex structure in a noisy real-valued time
%series is quantifiable and this type of complexity is directly
%correlated with predictability

\item that the way information is generated and processed by a system
  is correlated with the effectiveness of a given predictor on
  time-series data from that system

%\subitem [[Simply reminders not to be included]]We will have shown
%that the existence whether linear or nonlinear is picked up on with
%WPE but this point gets at whether the prediction model can use the
%structure or not (linear can't use nonlinear structure). The right to
%left shifts in \col and some of the \svd regimes and the lack of
%shift in \gcc illustrate this nicely.

%\subitem [[Simply reminders not to be included]]This is the linear vs
%nonliear vs random we see with the right to left shifts with lower
%complexity time series.

\end{enumerate}

%This portion should justify the following claim%%%%%%%%%%%%%%%%%%%%%%
%\item The existence of predictable structure in noisy real-valued time series is quantifiable by WPE and as a result WPE is correlated with prediction accuracy (MASE)  
%%%%%%%%%%%%%%%%%%%%%%%%%





%\item First paragraph: WPE is a good measure of predictability.  Figure:
%best athlete MASE vs. its WPE. 


%\item Quantifying structured and unstructured complexity is
%nontrivial in the case of real-valued noisy time series but WPE does
%this. [[talk about again here but justify in information theory
%section]]

The experiments below involve three different prediction methods
applied to time-series data from eight different systems: {\tt
  col\_major}, {\tt 403.gcc}, and the six different regimes of the
{\tt dgesdd} signal in Figure~\ref{fig:wwpe}.  The objective of these
experiments was to explore how prediction accuracy is related to WPE,
and how that relationship depends on the generating process and the
prediction method.  Working from the first 90\% of each signal, we
generated a prediction of the last 10\% using the \naive, ARIMA, and
LMA prediction methods, as described in Section~\ref{sec:accuracy},
then calculated the MASE value of those predictions.  We also
calculated the WPE of each time series using a wordlength of six, as
described in Section~\ref{sec:meaComplex}.  In order to assess the
run-to-run variability of these results, we repeated all of these
calculations on 15 separate trials: i.e., 15 different runs of each
program.  Figure~\ref{fig:wpe_vs_mase_best} shows the {\sl best} of
these 45 predictions for each system: i.e., the lowest error over all
15 trials and all three methods for each of the eight programs.  The
WPE is plotted against the corresponding MASE value here in order to
bring out the correlation between these two quantities.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{figs/prediction_vs_entropy}
  \caption{Weighted permutation entropy vs. mean absolute scaled error
    (MASE) of the best prediction of each system.
% 
% (across all trials and all prediction methods) 
% 
The dashed line is a least-squares linear fit of all the points except
{\tt dgesdd$_1$}, which we have excluded for reasons explained in the
text.  [[Ryan, is that correct?]]}
  \label{fig:wpe_vs_mase_best}
\end{figure}

The best-case prediction error for each of these eight systems is
roughly proportional to the weighted permutation entropy, which is
consistent with our first conjecture.  The dashed line in the figure,
a linear least-squares fit to these best-case errors\footnote{with the
  exception of {\tt dgesdd$_1$}, for reasons described below} captures
this rough proportionality.  This is not a formal result.  The three
methods used here were chosen to span the space of standard prediction
strategies, but they do not cover that space exhaustively.  Our goal
here is an empirical assessment of predictability and complexity, not
formal results about a ``best'' predictor for a given time series.
There may be other methods that produce lower MASE values than those
in Figure~\ref{fig:wpe_vs_mase_best}, but the sparseness of the points
in the upper-left and lower-right quadrants of this plot strongly
suggests that the underlying predictability of a time series is
inversely correlated with its WPE.  The rest of this section describes
our results in more detail, including the measures taken to assure
meaningful comparisons across methods, trials, and programs, and
elaborates on the meaning of the dashed line in the figure.

Figure~\ref{fig:wpe_vs_mase_all} shows WPE vs. MASE plots for all 360
experiments.
\begin{figure}
  \centering
    \includegraphics[width=0.6\textwidth]{figs/ARIMA_prediction_vs_entropy}
    \includegraphics[width=0.6\textwidth]{figs/ARIMA_prediction_vs_entropy}
    \includegraphics[width=0.6\textwidth]{figs/LMA_prediction_vs_entropy}
\caption{WPE vs. MASE for all trials, methods, and systems.  \svdone,
  \svdthree, and \svdfive are omitted from the top plot for scale
  reasons; their MASE scores are $2.676 \pm 4.328$, $31.386 \pm
  0.282$, and $20.870 \pm 0.192$ respectively.  {\color{red}Replace
    the top plot with a plot of the naive results---but running from 0
    to 3.1 on the horizontal axis and excluding SVD 1, 3, and 5}.  The
  dashed line is the same as in the previous figure [[Ryan, is this
      correct?]].  Numerical values, including means and standard
  deviations of the errors, can be found in Table~\ref{tab:error} in
  the appendix.
% 
%  [[All traces except those from \svd$_1$ lie above the line,
%      indicating that LMA is better suited prediction method for the
%      computer traces considered.]]  [[This comes {\bf long} before the
%      associated discussion in the text, and it's not what this figure
%      is trying to show here.]]
% 
}
    \label{fig:wpe_vs_mase_all}
\end{figure} 
There are 15 points in each cluster, one for each trial.  (The points
in Figure~\ref{fig:wpe_vs_mase_best} are the leftmost of the points
for the corresponding system in any of these three plots.)  The WPE
values do not vary very much across trials.  For most traces, the
variance in MASE scores is low as well, resulting in small, tight
clusters.  In some cases---ARIMA predictions of {\tt col\_major}, for
instance---the MASE variance is larger, which spreads out the clusters
horizontally.  The distribution of the cluster {\sl positions} is
quite different in the three plots: clusters of predictions generated
with the nonlinear LMA method are closer to the dashed line, whereas
the naive and ARIMA prediction clusters are more spread out [[need to
    revisit this wording when get the third image]].  This is a
geometric validation of the second conjecture in this paper, as
described in the following paragraphs.

Though \col is a very simple program, its dynamics are actually quite
complicated, as discussed in Section~\ref{sec:intro}.  Recall from the
graphical representations in the right-hand column of
Figure~\ref{fig:gcc_vs_col} that the \naive and ARIMA prediction
methods do not perform as well on this signal as the nonlinear LMA
method.  The MASE values of the \naive and ARIMA predictions are $0.57
\pm$ 0.001 and $0.59 \pm 0.21$, respectively, across all 15 trials.
That is, the errors produced by these methods are a little over half
as large as the errors produced by the random-walk method---a
primitive strategy that simply uses the current value as the
prediction.  However, the WPE value for the \col trials is $0.51 \pm
0.003$, which is in the center of the complexity spectrum described in
Section~\ref{sec:intro}.  [[This suggests that roughly half of the
    signal is predictive structure.]]  [[I'm still uncomfortable with
    that phrasing; let's revisit it after we have the terminology and
    definitions hammered out.]]  

This disparity---WPE values that suggest a high rate of forward
information transfer in the signal, but predictions with comparatively
poor MASE scores---is obvious in the geometry of the top two images in
Figure~\ref{fig:wpe_vs_mase_all}, where the \col clusters are far to
the right of the dashed line.  {\sl This indicates that these methods
  are not leveraging the available information in the signal.}  The
dynamics of \col may be complicated, but they are not unstructured.
This signal is nonlinear and deterministic, and if one uses a
prediction technique that is based a nonlinear model (LMA)---rather
than a method that simply predicts the running mean (\naive) or one
that uses a linear model (ARIMA)---the MASE score is much better:
$0.050 \pm 0.001$.  This prediction is 20 times more accurate than a
random-walk forecast, which is more in line with the level of
predictive structure that the low WPE value suggests is present in the
signal.  The dashed line is a heuristic that captures this argument.
It can allow practitioners to recognize when a prediction method is
not well matched to the task at hand: that is, when the time series
has more predictive structure than the method is able to use.  When
the predictions are well below that line (viz., the top images in
Figure~\ref{fig:wpe_vs_mase_all}), one should try a different method.

Note that the \col points are clustered in LMA but spread out
horizontally in ARIMA, far to the right of the dashed line (i.e., much
worse than the other methods).  [[We need to come up with a hypothesis
    about this spread and skew, even if it's totally tentative.  We
    may want to merge this discussion into one of the other \col
    paragraphs---or perhaps put it with the \svdone discussion later
    on?]]

The WPE of \svdfive ($0.677 \pm 0.006$) is higher than that of {\tt
  col\_major}.  This indicates that the rate of forward information
transfer of the underlying process is lower, but that time-series data
from this system still contain a significant amount of structure that
can, in theory, be leveraged to predict the future course of the time
series.  As mentioned above, though, in the discussion of {\tt
  col\_major}, the effectiveness of any prediction strategy will
depend on how well it leverages the available information; methods
that use linear models, for instance, may fail when applied to
nonlinear processes.
% 
% (This is the basis for the second conjecture above.)  
% 
The match between model and task is an issue in the \svdfive
experiments as well.  The MASE scores of the \naive and ARIMA
predictions for this system were $20.870 \pm 0.192$ and $2.370 \pm
0.051$, respectively: that is, 20.87 and 2.37 times worse than a
simple random walk forecast of the same signals\footnote{The \naive
  MASE score is large because the amount of variance in this signal is
  high, which makes guessing the mean a particularly bad strategy.
  The same thing is true of the \svdthree signal.[[Joshua, it isn't the variance it is the strength in the bimodal distribution and only predicting in one mode of the distribution...]]}.  Again, the
positions of these points on a WPE vs. MASE plot---significantly below
and to the right of the dashed line---should suggest to a practitioner
that there is more structure in this signal than the corresponding
method is able to leverage, and that a different method might do
better.  Indeed, for \svdfive, the LMA method produces a MASE score of
$ 0.718\pm 0.048 $ and a cluster of results that is much closer to the
dashed line on the WPE-MASE plot.  Again, this validates our second
conjecture: the LMA method can capture and reproduce the way in which
the \svdfive system processes information, but the \naive and ARIMA
prediction methods cannot.

The WPE of \gcc is much higher: $0.940 \pm 0.001$.  This system
transmits very little information forward in time and provides almost
no structure for prediction methods to work with.  [[What we see:
    \naive does the best, then LMA, then ARIMA.  Hypothesis: that that
    5\% of the structure is nonlinear, so LMA can use it better than
    ARIMA can.  \naive does better than LMA because it's filtering out
    the noise.]]

\svdone behaves very differently than the other seven systems in this
study: though its WPE is very high ($0.95 \pm 0.02$), two of the three
prediction methods do quite well, yielding MASE scores of $0.82 \pm
0.08$ (LMA) and $0.714 \pm 0.07$ (ARIMA).  This pushes the
corresponding clusters of points in the bottom two images in
Figure~\ref{fig:wpe_vs_mase_all} well above the trend followed by the
other seven clusters.  Also, the MASE scores of the predictions of
this system produced by the \naive method are highly inconsistent:
$2.67 \pm 4.33$.  These effects, we believe, are artifacts of the way
MASE is calculated.  Recall that MASE scores are scaled relative to a
random-walk forecast, which uses the current value as the prediction.
This strategy works very badly on signals with frequent, large, rapid
transitions.  Consider a signal that oscillates from one end of its
range to the other at every step.  A signal like this will have a low
WPE, much like {\tt col\_major}.  However, a random-walk forecast of
this signal will be 180 degrees out of phase with the true
continuation.  Since random-walk error appears in the denominator of
the MASE score, this effect can shift points leftwards on a WPE
vs. MASE plot, and that is exactly why the \svdone clusters for ARIMA
and LMA are above the dashed line in Figure~\ref{fig:wpe_vs_mase_all}.
This time series, which is shown in closeup in
Figure~\ref{fig:svdone-ts},
\begin{figure}[htbp]
  \centering
    \includegraphics[width=\textwidth]{figs/svdonets2}
\caption{A small portion of the \svdone time series}\label{fig:svdone-ts}
\end{figure} 
is not quite to the level of the worst-case signal described above,
but it still poses a serious challenge to random-walk prediction.  It
is dominated by a white-noise regime (between $\approx$1.86 and
$\approx$1.88 on the vertical scale in the Figure), punctuated by
short excursions above 1.9.  In the former regime, which makes up more
than 80\% of the signal, there are frequent dips to 1.82 and
occasional larger dips below 1.8.  These single-point dips are the
bane of random-walk forecasting.  In this particular case, roughly
40\% of the forecasted points are off by the width of the associated
dip, which skews the associated MASE scores.  Signals like this are
also problematic for the \naive prediction strategy, since the
outliers have significant influence on the mean.  This compounds the
effect of the skew in the scaling factor and creates a large spread in
the \svdone MASE values.  For all of these reasons, we view \svdone as
an outlier and exclude it from the fit calculation of the dashed line
in Figure~\ref{fig:wpe_vs_mase_best}.

%This portion should justify the following claim%%%%%%%%%%%%%%%%%%%%%%
%\item The way structure/information/complexity is processed internally by a given process plays a crucial role in predictability.
%%%%%%%%%%%%%%%%%%%%%%%%%

%In Figures~\ref{fig:lma_pred_vs_ent} and \ref{fig:arima_pred_vs_ent} we directly compare the performance of the LMA and ARIMA prediction methods (respectively) to the value of the weighted permutation entropy for all runs of each program under consideration. The LMA MASE values are largely similar to those of the best predictions, primarily because LMA often performed superior to ARIMA (and the na\"ive method). On the other hand, the ARIMA MASE values are largely uncorrelated with WPE values. As was hinted at while discussing the previous results, the fact that ARIMA is uncorrelated with WPE brings about an interesting perspective on information transfer and WPE. The WPE is sensitive to both linear and nonlinear structure. When you have a low WPE and a high ARIMA it could be that the structure WPE is picking up is simply nonlinear structure that LMA can handle but ARIMA cannot. So while ARIMA is consistently out performed by random walk,  there is plenty of structure present as suggested by WPE and taken advantage of by LMA but since it is nonlinear ARIMA can't take it into account and does bad.



%is in large part one of the major findings of this work. More specifically, say we tried to predict an arbitrary noisy real-valued time series with an ``out-of-the-box" prediction strategy like ARIMA as proposed in \cite{autoArima} and say we got inconsistent and bad forecasts, (i.e., perform worse than the na\"ive random walk strategy (MASE$>1$). How do we determine if the prediction strategy is not adequate for the prediction task, or if the signal is simply too complex to predict. If a signal is too complex and too little forward information transfer is present we may not be able to do better than the random walk, in which case we should not worry ourselves over finding a more complicated prediction strategy. However, if we measure the complexity to be low, $\textrm{WPE}<0.85$ (see Fig.~\ref{fig:pred_vs_wpe}) we can most likely do much better than the random walk and should search for more adequate prediction strategies. 


