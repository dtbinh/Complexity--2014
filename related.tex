\section{Related Work }\label{sec:related}

Modeling time-series data for the purposes of prediction dates back at
least to Yule's 1927 invention of autoregression~\cite{Yule27}.  Since
then, hundreds, if not thousands, of strategies have been developed
for a wide variety of prediction tasks.  The purpose of this paper is
not to add a new weapon to this arsenal, nor to do any sort of
theoretical assessment or comparison of existing methods.  Our goals
are focused more on the \emph{practice} of predication: \emph{(i)} to
empirically quantify the predictive structure that is present in a
real-valued scalar time series and \emph{(ii)} to explore how the
performance of prediction methods is related to that inherent
complexity.  It would, of course, be neither practical nor interesting
to report results for every existing forecast method; instead, we
choose a representative set, as described in Section~\ref{sec:model}.

Quantifying predictability, which is sometimes called ``predicting
predictability,'' is not a new problem.  Most of the corresponding
solutions fall into two categories that we call model-based error
analysis and model-free information analysis.
%
%Actually this first class are both just analyzing model error distributions but some are local and some are global models but both are really just error
%
%predicting local predictive capacity (radial basis functions stuff, trying to predict error bounds on next forecast based on ensemble uncertainty) but does not aggregate tell you at what level the time series exhibits complexity only locally predictive structure, this actualy gets at the interesting point that different regions of a time series may exhibit differnt levels of complexity which we will illustrate with \svd
%
The first class focuses on errors produced by a fixed forecasting
schema.  This analysis can proceed locally or globally.  The local
version approximates error distributions for different regions of a
time-series model using local ensemble in-sample
forecasting\footnote{The terms ``in sample'' and ``out of sample'' are
  used in different ways in the forecasting community.  Here, we
  distinguish those terms by the part of the time series that is the
  focus of the prediction: the observed data for the former and the
  unknown future for the latter.  In-sample forecasts---comparisons of
  predictions generated from \emph{part} of the observed time
  series---are useful for assessing model error and prediction
  horizons, among other things.}.
%
These distributions are then used as estimates of out-of-sample
forecast errors in those regions.  For example, Smith \emph{et al.}
make in-sample forecasts using ensembles around selected points in
order to predict the local predictability of that time
series~\cite{Smith199250}.  This approach can be used to show that
different portions of a time series can exhibit varying levels of
local predictive uncertainty.  We expand on this finding later in this
paper with a time series that exhibits interesting regime shifts.

Local model-based error analysis works quite well, but it only
approximates the \emph{local} predictive uncertainty \emph{in relation
  to a fixed model}.  It cannot quantify the inherent predictability
of a time series and thus cannot be used to draw conclusions about
predictive structure that may be usable by other forecast methods.
%
%Distrubtion of error. For many methods, if your error is not normally distributed this signals that there is stil more predictve structure to that could be used by for example a larger order ARMA process. But if error is normally distributed, this suggests that you have used up all the predictive structure that **that** model can use, this doesn't quantify if preidctuce structure exists that isn't being used by this process. For example, nonlinear structure which is ignored by a linear predictor.
%
Global model-based error analysis moves in this direction.  It uses
out-of-sample error distributions, computed \emph{post facto} from a
class of models, to determine which of those models was best.  After
building an autoregressive model, for example, it is common to
calculate forecast errors and verify that they are normally distributed.
If they are not, that suggests that there is structure in the time
series that the model-building process was unable to capture and use.
The problem with this approach is lack of generality.  
\label{page:normal-errors}
Normally distributed errors indicate that a model has captured the
structure in the data insofar as is possible, \emph{given the
  formulation of that particular model} (viz., the best possible
linear fit to a nonlinear dataset).  This gives no indication as to
whether another modeling strategy might do better.


A practice known as deterministic vs. stochastic
modeling~\cite{Casdagli92dvsplots, weigend-book} bridges the gap
between local and global approaches to model-based error analysis.
The basic idea is to construct a series of local linear fits,
beginning with a few points and working up to a global linear fit that
includes all known points, and then analyze how the average
out-of-sample forecast error changes as a function of number of points
in the fit. The shape of such a ``DVS" graph indicates the amounts of
determinism and stochasticity present in a time series.

The model-based error analysis methods described in the previous three
paragraphs are based on specific assumptions about the underlying
generating process and knowledge about what will happen to the error
if those assumptions hold or fail.  Model-free information analysis
moves away from those restrictions.  Our approach falls into this
class.  Our goal is to measure the inherent complexity of an empirical
time series, then correlate that complexity with the predictive
accuracy of forecasts made using a number of different methods.

We build on the notion of \emph{redundancy} that was introduced on
page~\pageref{page:redundancy}, which formally quantifies how
information propagates forward through a time series:
% how much information prior observations of a system lend to future
% values
i.e., the mutual information between the past $n$ observations and the
current one.
% took this out because DCE hasn't been introduced yet: 
% 
% For example,
% this would be the amount of information that delay coordinate
% embedding captures.
The redundancy of i.i.d. random processes, for instance, is zero,
since all observations in such a process are independent of one
another.  On the other hand, deterministic systems---including chaotic
ones---have high redundancy that is maximal in the infinite limit, 
and thus they can be perfectly predicted if observed for long 
enough~\cite{weigend-book}.  In practice, it is
quite difficult to estimate the redundancy of an arbitrary,
real-valued time series.  Doing so requires knowing either the
Kolmogorov-Sinai entropy or the values of all positive Lyapunov
exponents of the system.  Both of these calculations are
difficult---the latter particularly so if the data are very noisy or
the generating system is stochastic.

Using entropy and redundancy to quantify the inherent predictability
of a time series is not a new idea.  Past methods for this, however,
(e.g.,~\cite{Shannon1951, mantegna1994linguistic}) have hinged on
knowledge of the \emph{generating partition} of the underlying
process, which lets one transform real-valued observations into
symbols in a way that preserves the underlying dynamics~\cite{lind95}.
Using a projection that is not a generating partition---e.g., simply
binning the data---can introduce spurious complexity into the
resulting symbolic sequence and thus misrepresent the entropy of the
underlying system~\cite{bollt2001}.  Generating partitions are
luxuries that are rarely, if ever, afforded to an analyst, since one
needs to know the underlying dynamics in order to construct one.  And
even if the dynamics are known, these partitions are difficult to
compute and often have fractal boundaries~\cite{eisele1999}.

We sidestep these issues by using a variant of the \emph{permutation
  entropy} of Bandt and Pompe~\cite{bandt2002per} to estimate the
value of the Kolmogorov-Sinai entropy of a real-valued time
series---and thus the redundancy in that data, which our results
confirm to be an effective proxy for predictability.  This differs
from existing approaches in a number of ways.  It does not rely on
generating partitions---and thus does not introduce bias into the
results if one does not know the dynamics or cannot compute the
partition.  Permutation entropy makes no assumptions about, and
requires no knowledge of, the underlying generating process: linear,
nonlinear, the Lyapunov spectrum, etc.  These features make our
approach applicable to noisy real-valued time series from all classes
of systems.

