\section{Related Work }\label{sec:related}
{\color{blue}[[EDITABLE]]}
Attempting to model time series for prediction is an old problem which arguably began with Yule in 1927 when he invented autoregression \cite{Yule27}. Since then hundreds if not thousands of prediction strategies have been developed for a wide variety of prediction tasks. The purpose of this paper is \emph{not} to model a time series for prediction but to understand the predictability of a real-valued-scalar time series model free and then to associate the prediction accuracy with the time series inherent complexity. 

Quantifying predictability, or what is sometimes called predicting predictability, is also not a new problem and most solutions fall into two categories: model-based error analysis and model-free information analysis.

%Actually this first class are both just analyzing model error distributions but some are local and some are global models but both are really just error

%predicting local predictive capacity (radial basis functions stuff, trying to predict error bounds on next forecast based on ensemble uncertainty) but does not aggregate tell you at what level the time series exhibits complexity only locally predictive structure, this actualy gets at the interesting point that different regions of a time series may exhibit differnt levels of complexity which we will illustrate with \svd



The first class of methods is model oriented and focuses on either local or global in- or out-of-sample errors of a particular forecast schema. The local side of this tries to build error distributions for different areas of a time series reconstruction by doing local ensemble in-sample forecasting and then using the constructed distribution of in-sample errors as a means to predict out-of-sample forecast errors in that area of the time series reconstruction. For example, in \cite{Smith199250} in-sample forecasts are done for ensembles around ``data centers" (a subset of the time series) to predict the local predictability of a time series reconstruction. This does not however tell you if redundancy exists in a time series but only tells you the local predictive capacity in relation to this particular model (an extension of the model described in Section~\ref{sec:lma}). This paper does brings up a very interesting point however that different regions of a time series may exhibit varying levels of local predictive structure, we expand on this finding later in this paper with a time series called ``\svd".


%Distrubtion of error. For many methods, if your error is not normally distributed this signals that there is stil more predictve structure to that could be used by for example a larger order ARMA process. But if error is normally distributed, this suggests that you have used up all the predictive structure that **that** model can use, this doesn't quantify if preidctuce structure exists that isn't being used by this process. For example, nonlinear structure which is ignored by a linear predictor.



When analyzing post-facto global fits, practitioners often analyze the global error distribution of a forecasting schema. For example, it is common with autoregressive models to verify the error is normally distributed. If it is not then this is an indicator that more linear structure is present that can be used. The problem with this approach is that if the error is normally distributed it can not be said that the practitioner is using the ideal forecasting schema, e.g., if nonlinear structure is present that linear modeling is oblivious to. 


A practice known as deterministic vs. stochastic modeling\cite{Casdagli92dvsplots, weigend-book} bridges the gap between local and global error analysis. The basic idea is to construct a series of local linear fits (single point) to global linear fits (fitting all points known at that time), increasing the number of points in the fit slowly. Then analyze how the average out-of-sample error changes as a function of number of points in the linear fits. The shape of such a graph indicates how much determinism vs. stochasticity is present in a time series, see  \cite{Casdagli92dvsplots, weigend-book} for more information. 

 Our approach is completely different than this first class as it is model free. Each of the methods in the first class are based off very particular assumptions about the underlying generating system and knowledge of how error will result if those assumptions hold or break. Our approach instead empirically approximates the time series inherent complexity (Kolmogorov-Sinai entropy). Quantifying predictability through entropy/redundancy is not a new idea, but past methods (e.g.,\cite{Shannon1951, mantegna1994linguistic}) have hinged on knowledge of the \emph{generating partition} of the generating system. 
 
It is the case that the ideal method of computing the entropy of a system is via \emph{generating partitions}~\cite{lind95}, a technique which partitions observations into symbols in a way which preserves the underlying dynamics. Unfortunately, outside of one-dimensional unimodal maps and a few select higher dimensional maps, generating partitions are highly non-trivial to compute and often have fractal boundaries~\cite{eisele1999}. Furthermore, the underlying dynamic needs to be known to construct the generating partition. Unfortunately, using any partition (such as those produced by simply binning the data) other than the generation partition induces spurious complexity to the resulting symbolic sequence, thus misrepresenting the entropy of the underlying system~\cite{bollt2001}.

It seems desirable to quantify how much information prior observations of a system lend to future values, the \emph{mututal information} between, say, the past $n$ observations at the current one. This quantity is known as the \emph{redundancy}. For example, this would be the amount of information that delay coordinate embedding captures. For IID random processes,  the redundancy is zero, since all observations are independent of each other. On the other hand, any deterministic system --- including chaotic ones --- can be perfectly predicted if observed for long enough~\cite{{weigend-book}. Unfortunately it is quite difficult to estimate the redundancy in practice for an arbitrary, real-valued time series. Doing so requires knowing either the Kolmolgorov-Sinai entropy or the Lyapunov spectrum. As discussed above, computing the entropy is quite difficult. Similarly, computing the Lyapunov exponent from an arbitrary time series is also fraught with difficulties, and is effectivly impossible if there is significant noise or the generating system is stochastic.

Our approach expands on \cite{bandt2002per} to approximate normalized redundancy of a time series as a proxy for predictability. This method is different from the current literature because it does not rely on generating partitions, and as such does not suffer from the previously mentioned biases and no knowledge of the underlying generating process is necessary, e.g, linear, nonlinear, the Lyapunov spectrum, etc. In contrast our approach uses no assumption about the underlying generating process, does not require generating partitions, and is applicable to noisy real-valued time series from all classes of systems.


{\color{red}
Maybe talk about ``
wpe has been applied to predicting irregularities in brain wave data but no on one has examined it's correlation with predictive structure. "
but kind of puts the cart before the horse

[[ryan]] i tried writing a paragraph about this around 7 times, and couldn't get anything that sounded reasonable.
}

