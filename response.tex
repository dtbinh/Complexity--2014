\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{graphicx}

\newcommand{\alert}[1]{{\color{red}#1}}

\title{Response to the Reviewers}
\author{Joshua Garland, Ryan James, and Elizabeth Bradley}
\date{\today}

\begin{document}

\maketitle

\section*{Response to the First Referee}

\emph{In this manuscript the authors present a way to explore the
  relationship between predictability and complexity across a broad
  (but not complete) array of forecast strategies such as random walk
  (last value), na\"ive (average value), regression based (ARIMA) and
  a nonlinear method based on state space reconstruction (LMA). They
  use weighted permutation entropy as a criterion/index of predictive
  structure in the time series and illustrate their approach on eight
  real-world datasets derived from three different systems (computer
  performance traces).}

\emph{Overall the manuscript is well written and clearly
  understandable. The analysis seems to be carried out carefully and
  the conclusions drawn are quite reasonable. While I don't get the
  impression that this is a breakthrough paper I can still recommend
  the manuscript for publication in PRE if the issues listed below are
  addressed thoroughly.}

We thank the reviewer for their kind words and appreciate the time
they put into reviewing our work.

\noindent\emph{Major issues:}

\emph{I don't get the reason for normalizing the three methods by the
  random walk method (for example in Table I). As the authors write
  themselves this makes the indicator vulnerable to a bias caused by
  the high influence of this (arbitrarily selected) reference
  method. Why not just use a non-referential estimator, which then
  would also allow to compare the inherent predictability of different
  systems (as it is now the values obtained for each systems are
  strongly influenced by the random walk estimate on that particular
  system and thus can not be compared). This will also change the
  results of the fitting (the authors explicitly exclude one value
  because of the effect caused by the normalization). And it is not
  that the random walk predictor is the one standard method that all
  other methods have to compete against.}

No error metric is perfect.  For our purposes, we needed one that was
scale free and comparable across different time series.  While MASE
has its drawbacks, it was a very good fit to those requirements.  In
our opinion, this outweighed its disadvantages.  There is a great deal
of discussion of this cluster of issues in [Hyndman and Koehler, 2006]
(our reference \alert{[[fill in]])}.  If the reviewer thinks that it
would strengthen our paper, we can recapitulate some of that material
in our Section IVD.

\emph{In many parts the article reads almost like a review. There is a
  lot of unnecessary redundancy (the entropy rate is quite low). In
  particular, the main point (``The aim of the paper is not ... but
  ...'') is repeated far too many times. The paper while easy to read
  could be shortened considerably without any loss of information.}

Due to the broad selection of fields that this work draws upon, we
thought it necessary to provide background for readers who might not
be familiar with all of them.  While this necessitated some
redundancy, we have made an effort to trim the fat and make the review
portions more streamlined.

\noindent\emph{Minor issues:}

\emph{Page 1: Why mention the `halting problem'? I don't see a
  straightforward connection to the problem at hand. The only
  connection is `undecidability' but there are many other examples for
  that.}

This is an important issue to the computer-science community, but we
have taken the reviewer's advice that it is less so for Physical
Review E---and hence removed it.

% Our intent in mentioning this theoretical result was to side-step the
% possible response of simply ``always use the most sophisticated
% prediction scheme''. This corollary implies that no matter how
% sophisticated and broad a scheme we construct, it will not be the
% optimal one for some time series. Thus there is a real need for
% determining when a better scheme exists. \alert{We have modified the
%   paragraph to make this point more clear.}

\emph{Page 10: `error between the predictions and the true continuations'
$\rightarrow$ `error of the predictions with respect to the true continuations'
Figure 6 on Page 11: It would be more intuitive to turn this into a `table of
figures' with the four methods as row labels on the left and the three systems
as column labels at the top. Also: Caption `for forecast of ... and all four
prediction strategies'.}

We have modified the text as the reviewer suggested, and agree that it reads
more clearly now. We have also modified the figure as suggested.

\emph{First equation on Page 14: Shouldn't the index $j$ run from $i+1$ to
$i+\ell$ (instead of from $i$ to $i+\ell$)? You normalize by $\ell$ (not
$\ell+1$) values.}

Thank you for catching our mistake; we have corrected the equation.

\emph{Page 19: ``There are some exceptions: [...] Conversely, forecast errors
for the \texttt{col\_major} and \texttt{dgesdd$_5$} signals are higher than the
corresponding WPEs suggest --- except for the nonlinear LMA predictor.'' This is
not really an exception. As mentioned by the authors before, in general, the
result only holds under the assumption that a reasonably good model has been
used for the prediction. But for ill-matched datasets and models like in the
cases mentioned above this is just not the case so it is not a surprise that the
forecast errors are higher.}

You are absolutely correct and we've adjusted the text accordingly.

\section*{Response to the Second Referee}

We thank the reviewer for taking the time to read our paper and
provide us with valuable feedback.

\emph{The problem of time series prediction is considered in the paper
  of J.  Garland et al. Namely, the authors suggest to quantify
  predictability in advance (before using any concrete prediction
  scheme) based on the complexity estimate (weighted permutation
  entropy, WPE). They hypothesize that WPE is related to the best
  possible prediction error in a simple way. For several benchmark
  time series (reflecting variations of a computer performance during
  execution of different tasks), they obtain estimates of WPE and
  prediction error (the least one over four techniques), and build a
  linear regression using seven or eight resulting data points. They
  suppose that the obtained linear function can be used to ``predict''
  the best prediction error from the model-free WPE estimate.  Thus,
  the authors finally assume (as could be seen from the last section)
  that WPE and optimal prediction error for all systems are related
  via the same universal linear function with fixed coefficients. This
  is what I can understand from the entire paper, though the problem
  is not so definitely described in the Introduction.}

We have modified the introduction in order to define our goals more
clearly.

\emph{I think that the possibility to estimate the best prediction
  error in advance from a relatively simple WPE characteristic would
  be interesting and useful, if it could hold true for a wide class of
  systems. Such a question would deserve a careful study as that
  started in the presented work. However, I have a number of critical
  remarks and strongly doubt that the conjecture of the authors can be
  valid for a reasonably wide class of systems (not even speaking of
  ``all systems''). Other aspects of the paper are not so important,
  in my view, despite some of them are fresh and interesting
  (e.g. taking a computer as an object of prediction). My critical
  remarks are listed below.}

Computer dynamics spans the full range of behavior reported in the
nonlinear dynamics literature.  Standard nonlinear time-series
analysis of data from these systems has confirmed the existence of
fixed points, periodic orbits of different periods, chaotic
trajectories with various Lyapunov exponents, etc.  in their dynamics.
While it is certainly possible that the nonlinear dynamics of computer
performance is subtly different from the nonlinear dynamics of other
systems---in a manner that cannot be detected by traditional nonlinear
time-series analysis---we believe that our results will hold for time
series drawn from other physical systems.

We are certainly aware that this claim may not be true, however, and
we are in the process of the follow-up study suggested by the
reviewer.  We have found that the relationship posited in our paper
holds for a time series of voltages from a chaotic laser used in the
SFI prediction competition, for instance.  (It also holds for various
paradigmatic systems, such as the Henon map and the Lorenz system, as
discussed in a later section of this response.)  The image below is an
augmented version of our revised Figure 7 that includes all of these
examples.

\begin{center}
    \includegraphics[width=0.8\columnwidth]{figs/new_prediction_vs_entropy_extras}
\end{center}

\noindent The dashed curve in this plot (and in Figure 6) was fit
\emph{solely on the computer performance data}.  Nonetheless, the
points from the other time series fall within a narrow band around the
fit.  Note that this fit ($y = a \log(b x + 1)$) is different than in
the submitted version.  A careful examination of the data, prompted by
the reviewer's questions, suggested that this was a better match to
the data; theoretically, too, a log fit is more appropriate because
any signal with zero entropy should be perfectly predictable, and so
should ideally have a MASE value of zero.  Finally, note that there
are a lot more points on this plot than in our original version; this
is discussed further below.

\smallskip

\emph{1) Introduction is too long, but the main problem is not
  formulated definitely enough. I see that the main point is the
  approximately linear (or at least one-to-one) relationship between
  WPE and the best prediction error. This aim is not claimed
  explicitly in the Introduction. All other aspects of the manuscript
  do not seem important enough to be published in a separate article.}

We have shortened the introduction and modified the manuscript
throughout to make it more clear that our goal is to derive a
practical criterion for best-case prediction error in a model-free
way, and to test it in the context of empirical data.

\smallskip

\emph{2) In particular, two primary findings mentioned on page 2
  (Introduction) are formulated quite vaguely. The first part of the
  first finding reads ``(i) complexity of a noisy real-value time
  series is quantifiable by permutation entropy''. However, it seems
  obvious from the previous research, including that cited by the
  authors. The authors do not suggest PE or WPE here, they exploit
  them as previously known approaches. It was previously known that PE
  is an estimate of KS entropy as the authors also state. It was well
  known that KS entropy relates to predictability and, in this sense,
  it is a measure of complexity (see e.g. the work [G. Boffetta,
    M. Cencini, M. Falcioni, and A.  Vulpiani, ``Predictability: a way
    to characterize complexity'' // Physics Reports 356 (2002)
    367–474] which is not cited by the authors). Thus, the first part
  of the first finding does not provide any new information. The
  second part of the first finding reads ``(ii) complexity of the
  noisy real-valued time series is correlated with prediction accuracy
  of an appropriate predictor''.  However, it seems also obvious and
  directly follows from the definition of KS- entropy and its
  discussion in many papers including [Boffetta et al.] cited above.}

These claims have indeed been explored extensively in theory and in
toy models.  Our specific claim, which was not well explained in the
submitted version, is that those claims also hold---and can be
practically applied---in real-world time-series data that may contain
dynamical and/or observational noise.  This has not, to our knowledge,
received much attention in the literature.  We have added a paragraph
to the introduction that hopefully makes all of this more clear.

We appreciate being informed about the Boffetta et al.  paper, and we
have included a discussion of it at the end of the related-work
section.

\smallskip

\emph{3) By the way, the authors call ``fully complex'' the white
  noise process, i.e. they equate predictability and
  complexity. However, they do not even mention another approach to
  the notion of complexity which ascribes low complexity to the white
  noise [the works e.g. by C.R. Shalizi and J. Crutchfield carried out
    in the Santa Fe Institute from where one of the authors is]. It
  would be appropriate to mention that ambiguity of the notion of
  complexity if the latter plays an important role in many
  formulations of the authors.}

We have added a short discussion about the differences in notions and
definitions of complexity to the introduction and related-work
section, and clarified the working definition that we use in this
paper.

\smallskip

\emph{4) The second finding reads ``The way information is generated
  and processed internally by a system plays a crucial role in the
  success of different forecasting schema - and in the choice of which
  one is appropriate for a given time series''. This statement also
  looks trivial. Indeed, an optimal predictor for a linear stochastic
  system and low-dimensional nonlinear deterministic system are quite
  different (an AR model versus a local or global nonlinear model) due
  to different properties of the original systems. Thus, there is
  nothing to be proven here. Probably, the authors imply their
  particular benchmark signals (reflecting a computer behavior) but
  they do not claim that.  Thus, both primary findings seem to provide
  nothing new.}

The reviewer is absolutely right.  This finding, as it was originally
stated, is indeed trivial.  Our intent, as we have modified the text
to reflect, is that the appropriateness of a predictor can be
discovered \emph{empirically}, via comparison of its MASE value with
its WPE value.  Previous explorations of predictability have used
clean data and/or systems where the generating process is known.  Our
work makes neither of these assumptions.

Given a linear stochastic system, a linear stochastic prediction model
should certainly be more productive than a nonlinear deterministic
prediction model.  But in real-world data, the generating process is
almost never known, and it can be very hard to evaluate a model that
one has constructed.  Standard methods like the AIC approach used in
ARIMA modeling make a choice from among a large class of possible
models.  These selection algorithms rarely have a ``none of these are
appropriate'' output; they simply choose the model that best satisfies
their built-in criteria---even if none of the models in the class are
really appropriate.

The challenge for the practitioner is to know whether there is a
mismatch between a particular time series and a particular forecast
model: i.e., whether one can do better and should try another model,
or whether the time series is so complicated that the model is doing
as well as one could expect.  The WPE vs. MASE curve in our paper is
an empirical heuristic that should help practitioners do that.

We appreciate the reviewer alerting us to our muddy wording and
thinking about these issues.  We have reworked the findings to clarify
matters and also added an explanatory paragraph directly following
those findings.  While this lengthened the introduction, we feel that
it has focused and clarified the contribution of the work.

\smallskip

\emph{5) The benchmark system - a computer as an object of prediction
  - seems to be an unexpected and, probably, interesting choice but
  just as an additional illustration of a statement which should be
  first shown for well-controlled and well-understood paradigmatic
  systems (like low-dimensional maps or ordinary differential
  equations). Indeed, it is not known in advance what error one should
  expect for such a complicated object using a certain predictive
  technique, what technique is optimal or close to optimal, etc. If
  the main point of the research is the relationship between WPE and
  the best prediction error (if any), selection of such a complicated
  object as computer performance variations only introduces its own
  difficulties into the problem.}

Computers are indeed complicated objects, but we have studied their
dynamics extensively and established quite firmly that they are
garden-variety dynamical systems with surprisingly low-dimensional
dynamics [Mytkowicz et al. CHAOS, 2009].  Standard nonlinear
time-series analysis techniques on performance data measured from
running computers produce clean and consistent results, as mentioned
above: fixed points, bifurcations, chaotic attractors, and so on.

Complicated, unknown systems are where prediction really
matters---especially systems where a good prediction can make a
practical difference (e.g., reallocation of resources in a computer to
suit the dynamically changing needs of a program).  Well-known systems
are certainly good testcases.  As is also mentioned above, the
relationship posited in our paper does indeed hold for paradigmatic
systems like the Henon map and the Lorenz equations.  We had not
included these examples in our paper because they simply confirmed
well-known results, and because our focus was on confirming those
results in the context of noisy, real-world data.  \alert{[[Liz to do,
      then finish this wording: We have added a sentence in the
      conclusion (or maybe end of discussion) about the fact that the
      WPE vs. MASE relationship for these synthetic examples follows
      the empirically determined pattern in our paper.]]}

\smallskip

\emph{6) The description of the choice of the benchmark system in the
  Introduction is too long. On the other hand, if the computer signals
  are the main interest of the work, then it should be claimed
  explicitly. Then, the paper should be rewritten and submitted to a
  kind of engineering journals where it might well be of interest.}

The primary goal of this paper is to explore the relationship between
prediction error and WPE in noisy, real-world systems, and the
practical applicability of this relationship.  Accordingly, we have
cut down on the level of detail regarding the benchmark system and
pointed interested readers to our other papers for the engineering
details.  We did not feel that removing all of this material would
be effective, however, because it helps the reader understand why
this is a good testbed for exploring the problem at hand.

\smallskip

\emph{7) As for the choice of the four prediction techniques, some of
  them seem superfluous. E.g. naive approach is just a global AR model
  of the zero order.  Random-walk is similar to the AR model of the
  first order where the coefficient is not estimated by the
  least-squares but set equal to 1 (or this is an ARIMA model with the
  first difference and ``zero ARMA part''). Inefficiencies of the
  ARIMA model as compared to those choices in the paper can represent
  an inappropriate procedure for the order selection rather than
  indicate inappropriateness of the entire ARIMA approach. Thus,
  linear ARIMA and nonlinear LMA might be sufficient (in combination
  with mathematical benchmark examples) to represent close-to-optimal
  prediction errors.}

It is certainly true that the naive and random-walk predictors can be
seen as subclasses of the ARIMA model family.  We have explicitly
included them, however, because they are used (sometimes to great
success) in practice and because that made for a more comprehensive exploration.

The {\tt auto.arima} method that was used to build all of the ARIMA
models in this paper---another ``off the shelf'' method in the
modeling and prediction communities---follows the procedure outlined
in Section V B to choose values for the model parameters.
Sometimes the choices made by that procedure (e.g., setting a
particular parameter to zero) produce a naive or random-walk
predictor.  Our method can help practioners know whether this is a
good or bad thing.  If a particular model performs poorly (compared to
what the WPE value might suggest), it may be an indication that the
chosen parameter values were suboptimal---or even that no member of
the ARIMA family will work and one should try a nonlinear model.

% Unfortunately the WPE value alone is insufficient to differentiate
% these two situations.  It can identify both, but cannot distinguish.

We have changed the term used in the paper to {\tt auto.arima} in
order to make it clear that this refers not to the general class of
these models, but to a particular member of that class chosen via a
particular procedure.

\smallskip

\emph{8) Conditions for inefficiency of simple prediction schemes
  (like naive and random-walk) discussed in the paper look rather
  obvious as well and do not give new information. Is it the purpose
  of the paper to discuss when and where a concrete simple prediction
  scheme fails?}

This short paragraph in this paper, which explains the efficiencies
and inefficiencies of these schemes, was present for two purposes:
first, to explain why we have included these simple prediction schemes
and not just the more-complicated ones; second, to help in explaining
the specific results described later in the paper.

\smallskip

\emph{9) A better justification of the prediction error metrics could
  be also in order if this work is continued. At least, comparison of
  prediction errors quantified with different metrics seems
  necessary. Especially, taking into account the following major
  concerns.}

\alert{Liz to do: We have extended the justification for our use of
  the MASE metric.}  The paper in which MASE was introduced---[Hyndman
  and Koehler, 2006], our reference \alert{[[fill in]]}---includes an
extensive evaluation of this metric, as well as a comprehensive
comparison to other metrics.  If the reviewer thinks that it would
strengthen our paper, we can recapitulate more of that material in our
Section IVD. As part of our current work on this topic, we are
evaluating how other choices of metric affect the shape of our curves,
but we feel that a comprehensive study of this issue is best left as a
sequel to this work.

\smallskip

\noindent\emph{General remarks.}

\emph{I) Strict relationship between WPE and prediction error
  applicable to all systems is hardly possible. In particular, there
  is a notion of epsilon-entropy which tends to the KS entropy if
  epsilon (the cell size) tends to zero (see e.g.  [Boffetta et al.]
  cited above). For stochastic systems, the KS entropy is infinite and
  epsilon-entropy varies strongly with the scale epsilon. Seemingly,
  the permutation entropy estimate in this case will strongly depend
  on the choice of the word length L. The authors concentrate on L = 6
  and obtain coefficients of the regression between WPE and prediction
  error only for that choice and for their particular
  signals. However, other systems and other choices of L may easily
  lead to other regression coefficients so that deciding about the
  best possible prediction error from WPE seems impossible in
  general. It is good if such a decision is possible for a certain
  narrow class of systems. However, the authors do not even discuss to
  which class of systems their results apply.}

WPE values depend not only on wordlength, but also on \emph{data}
length---since one needs more data to get good statistics on the words
for the PE calculation.  These two interacting knobs are not easy to
set.  Perhaps as a result of this, wordlength choices are rarely
justified in the permutation entropy literature.

The extent of the discussion in [Bandt \& Pompe, {\sl PRL}, 2002], for
instance---the paper in which PE was proposed---is ``Our entropies are
calculated for different [$\ell$] but we do not to determine a limit
for large [$\ell$], although this is an interesting theoretical
problem.  ``For practical purposes, we recommend 3\dots7...''  The
corresponding discussion in [Cao {\sl et al.}, {\sl PRE}, 2004] reads
``In their paper, Bandt \& Pompe recommended [$\ell$] of 3-7.  We
often found that 3 and 4 may still be too small, and a value of 5, 6,
or 7 seems to be the most suitable....''

The right thing to do, as we argued in Section [[what]], is to compute
PE across a \emph{range} of wordlengths ($\ell$) and look for a flat
spot in the curve of PE versus $\ell$.  We used that method to choose
individual $\ell$ values for each time series studied here.  We
conjecture that a systematic approach like sill mitigate any variation
in the results, thereby preserving the shape of the WPE vs. MASE
curve.  We have not yet confirmed that conjecture, but we are in the
process of doing so.

Regarding generality: the goal of this paper was to explore model-free
heuristics for empirical time-series data, where one does not know
anything about the underlying system.  In situations like this,
equivalence classes are difficult to define.  We addressed
this---insofar as is possible in an experimental context---by choosing
a range of time-series data sets whose behavior spans a wide range of
dynamical behaviors.  For all of these examples---as well as for the
39 additional ones\footnote{Henon, Lorenz, SFI A, random walk,
  nonlinear transformations of the 30 [[svdtwo]] and [[svdsix]]
  signals} constructed for this rebuttal letter---the pattern in
Figure 7 remained virtually unchanged, as described in the following
entry.  This strongly suggests, but of course does not prove, that our
approach is general.  As the reviewer suggested, this will require
extensive study and evaluation in future work.

\smallskip

\emph{II) PE is invariant under an invertible nonlinear change of
  variables. The prediction error measured as the authors suggest is
  not invariant. Thus, such a simple change of variables can lead, at
  least, to a change in regression coefficient between WPE and
  prediction error. Thus, the authors should also discuss to which
  variable for that class of systems their result apply. I think that
  the fact that the authors observe a certain regression line at all
  seems to be due to small number of different signals they consider
  (only 7 or 8 different data points, such a small number of points
  may lie approximately near a straight line by chance).}

\alert{Liz to fix following text, now that we have these experiments}

PE is invariant under an invertible nonlinear transformation of the
values in the time series, but WPE is not.  But MASE varies \emph{in a
  complementary way}.  Ryan will do $x(t)$ and $5e^{x(t)}$ and show
that it's on the curve.

% Ryan will insert figure here with x(t) = svdone and svdsix
\begin{center}
    \includegraphics[width=0.8\columnwidth]{figs/nonlinear_transform_data}
\end{center}

This is consistent with our claim of generality; this is just another
time series.  A xform like this effectively changes the generating
process, but our model-free result handles it.

The fit of the regression line was not a function of the small number
of points; when we included all 105 WPE-MASE pairs, rather than just a
representative from each program, the slope of the fit changed by 2\%
and the y intercept by 1\%.

We have revised the Figure, and the associated text, to include all
105 points.  We have also moved from a linear fit to a log fit in this
curve to better match the theoretical relationship between MASE and
WPE, as mentioned above.  Finally, we now show a ``fuzzy'' version of
the fitted curve in an effort to emphasize that the fundamental claim
of the paper is not related to the crispness of the fit.

\alert{Need the change and the confidence stuff for this: Fit changed
  by [[what]] when we redid it with these 34 new points.}  It now
reflects a map, a flow, a purely stochastic signal, a variety of
real-world data from two different systems (laser, computer), and
nonlinear transformations of that real-world data.  Seems pretty
compelling.

\emph{To summarize, the paper shows that the authors are well familiar
  with the entire topic and highly qualified, they overview some
  well-known facts (WPE, various prediction techniques, etc) and
  ``invent'' an unexpected test system (computer performance data),
  they report about their attempts to play around with all that
  staff. However, their results do not seem to provide any new
  information to a specialist. Probably, this work can be continued to
  get some more reliable results about concrete relationship between
  WPE and prediction error for certain classes of systems (however, I
  doubt that it is possible to derive any simple and widely applicable
  relationship like the linear function suggested by the
  authors). Seemingly, it should then be a new paper which can be
  hardly considered as a revised version of the present manuscript.}

It is our hope that these clarifications, the attendant modifications
to the paper, and the additional preliminary results that we included
in this rebuttal letter have convinced the reviewer that this
manuscript is appropriate for publication in PRE.

Because of the depth and extent of the revisions that we made in
response to this reviewer's comments, we did not feel that it was
effective to highlight the changes in a different color in the
manuscript; please let us know if a more-detailed description of the
changes would make the re-reading easier.


\end{document}
